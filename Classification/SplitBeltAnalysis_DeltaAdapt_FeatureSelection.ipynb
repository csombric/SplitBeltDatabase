{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection for Classification of DeltaAdapt Success\n",
    "---\n",
    "In this notebook, single, two-way, and three-way interactions of features useful for classifying DeltaAdaptation success are identified.\n",
    "\n",
    "** DeltaAdapt **\n",
    "\n",
    "DeltAdapt indicate the amount that subjects changed the way the walk during training. A non-zero DeltaAdapt means that subjects were able to change the way they walked during training, whereas DeltAdapt values close to zero indicate minimal changes in gait during training. Although DeltaAdapt is continuous, here we are classifying those whose gait training was successful (i.e., gait changed due to training) versus those whose gait training was not successful (i.e., no change in gait due to training).\n",
    "\n",
    "**Feature Selection Methods**\n",
    "\n",
    "*Random Forest:*  A Random Forest will be used to identify important features.\n",
    "\n",
    "*Gradient Boosting:*  Gradient Boosting was utilized in order to calculate Friedman and Popescuâ€™s H statistics to indentify important 2-way and 3-way interactions.\n",
    "\n",
    "** Selected Features**\n",
    "Single Features:\n",
    "- AdaptationDuration\n",
    "\n",
    "Two-Way Feature Interactions:\n",
    "- {('SpeedDifference', 'MidSpeed'): 0.2517395510368736}\n",
    "\n",
    "Three-Way Feature Interactions:\n",
    "- None\n",
    "\n",
    "Reference Material:\n",
    "- https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "- https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb\n",
    "- https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Random%20Forest%20Explained.ipynb\n",
    "- https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Loadind Data\n",
    "import matplotlib.pyplot as plt  # To visualize\n",
    "\n",
    "df = pd.read_csv(\"CleanDataBase.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed to increase repeatability\n",
    "import numpy as np\n",
    "RSEED=50\n",
    "np.random.seed(RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Features and Targets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "\n",
    "Y, X = dmatrices('DeltaAdaGood ~ C(SpeedRatio) + SpeedDifference + MidSpeed+ C(Abrupt) + MidBase + \\\n",
    "               AdaptationDuration + Age + C(Young) + Height + Weight + BMI + C(IsCatch) + C(Stroke)', df, return_type=\"dataframe\")\n",
    "\n",
    "feature_cols = ['C(SpeedRatio)[T.3.0]', 'SpeedDifference', 'MidSpeed', 'C(Abrupt)[T.1]', 'MidBase', \\\n",
    "               'AdaptationDuration', 'Age', 'C(Young)[T.1]', 'Height', 'Weight', 'BMI', 'C(IsCatch)[T.1]', 'C(Stroke)[T.1]']\n",
    "\n",
    "\n",
    "#feature_cols = ['C(SpeedRatio)[T.3.0]', 'SpeedDifference', 'MidSpeed', 'C(Abrupt)[T.1]', 'MidBase', \\\n",
    "#               'AdaptationDuration', 'Age', 'C(Young)[T.1]', 'Height', 'BMI', 'C(IsCatch)[T.1]', 'C(Stroke)[T.1]']\n",
    "\n",
    "\n",
    "\n",
    "#feature_cols = ['C(SpeedRatio)[T.3.0]', 'SpeedDifference', 'MidSpeed', 'C(Abrupt)[T.1]', 'MidBase', \\\n",
    "#               'AdaptationDuration', 'Age', 'Height', 'BMI', 'C(IsCatch)[T.1]', 'C(Stroke)[T.1]']\n",
    "\n",
    "\n",
    "target_cols = [\"DeltaAdaGood\"]\n",
    "\n",
    "df_Interactions = pd.concat([X,Y], axis=1)\n",
    "\n",
    "Features = df_Interactions[feature_cols]\n",
    "Target = df_Interactions[target_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data into Training and Testing Sets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(Features,Target,test_size=0.2, random_state = RSEED)\n",
    "\n",
    "y_test=y_test.values.ravel()\n",
    "y_train=y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up Sampling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # Also to visualize\n",
    "sns.countplot(x = \"DeltaAdaGood\", data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os = SMOTE(random_state=0)\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=feature_cols)\n",
    "os_data_y = pd.DataFrame(data=os_data_y,columns=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of BadAE in oversampled data\",len(os_data_y[os_data_y[\"DeltaAdaGood\"]==0]))\n",
    "print(\"Number of GoodAE\",len(os_data_y[os_data_y[target_cols]==1]))\n",
    "print(\"Proportion of BadAE data in oversampled data is \",len(os_data_y[os_data_y[\"DeltaAdaGood\"]==0])/len(os_data_X))\n",
    "print(\"Proportion of GoodAE in oversampled data is \",len(os_data_y[os_data_y[\"DeltaAdaGood\"]==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = os_data_X\n",
    "y_train = os_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(random_state=RSEED, max_depth=4)\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and Visualizing the Un-Optimized Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def SaveAndVisualizeForest(model, figName, FeatureNames):\n",
    "    estimator = model.estimators_[1]\n",
    "\n",
    "    export_graphviz(estimator, out_file = figName + '.dot', feature_names = FeatureNames,\n",
    "                rounded = True, precision = 1, class_names = ['goodDA', 'badDA'], filled = True)\n",
    "\n",
    "\n",
    "    # Use dot file to create a graph\n",
    "    (graphy, ) = pydot.graph_from_dot_file(figName + '.dot')\n",
    "\n",
    "    # Write graph to a png file\n",
    "    graphy.write_png(figName + '.png'); \n",
    "    \n",
    "    #Visualize for notebook\n",
    "    img = mpimg.imread(figName + '.png')\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(30,10)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveAndVisualizeForest(model, 'DAForest', feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual class predictions\n",
    "rf_predictions = model.predict(X_test)\n",
    "\n",
    "# Probabilities for each class\n",
    "rf_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training predictions (to demonstrate overfitting)\n",
    "train_rf_predictions = model.predict(X_train)\n",
    "train_rf_probs = model.predict_proba(X_train)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Evaluation: Confusion Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt  # To visualize\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, rf_predictions)\n",
    "plot_confusion_matrix(cm, classes = ['Poor SS', 'Good SS'],\n",
    "                      title = 'Steady State Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Evaluation:  ROC & Precision/Recall Curves\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tagen from: https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot formatting\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "def evaluate_model(predictions, probs, train_predictions, train_probs, train_labels, test_labels):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, \n",
    "                                     [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, \n",
    "                                      [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); \n",
    "    plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection: Single Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "fi = pd.DataFrame({'feature': list(X_train),\n",
    "                   'importance_unop': model.feature_importances_})\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(fi.feature, fi.importance_unop, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(fi.feature, list(X_train), rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this changes when you re-run the model the following reatures seem to be the most important (importance >0.2):\n",
    "- AdaptationDuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Feature Selection: Interactions Identified with Friedman and Popescuâ€™s H statistics\n",
    "---\n",
    "\n",
    "In order to compute the H statistic I must use a Boosting Classifier, I will be using Gradient Boosting Classifier.  I will be accessing 2-way and 3-way interactions seperately.\n",
    "\n",
    "References:\n",
    "- https://christophm.github.io/interpretable-ml-book/interaction.html#implementations <br />\n",
    "- https://pypi.org/project/sklearn-gbmi/  <br />\n",
    "- https://github.com/ralphhaygood/sklearn-gbmi/blob/master/example.ipynb <br />\n",
    "- https://blog.macuyiko.com/post/2019/discovering-interaction-effects-in-ensemble-models.html <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_gbmi import *\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "gbr_1 = GradientBoostingRegressor()\n",
    "gbr_1.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Way Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the two-variable H statistic of each pair of predictor variables. \n",
    "TwoWay_HStat = h_all_pairs(gbr_1, X_train)\n",
    "\n",
    "{k: v for k, v in TwoWay_HStat.items() if v >= 0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tope 2 Most Significant 2-way Interactions (>0.25):**\n",
    "- {('SpeedDifference', 'MidSpeed'): 0.2517395510368736}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Features, Target.values.ravel())\n",
    "\n",
    "plt.figure()\n",
    "features = [(1, 2)]\n",
    "plot_partial_dependence(clf, Features, features) \n",
    "plt.gcf()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Three way interactions are the largest interaction that I can quantify with this methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3-Way Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "FeatureName = list(X_train)\n",
    "Combos = list(combinations(range(len(FeatureName)),3))\n",
    "\n",
    "for x in range(len(Combos)):#len(Combos):\n",
    "    ThreeNames = [FeatureName[Combos[x][0]], FeatureName[Combos[x][1]], FeatureName[Combos[x][2]]]\n",
    "    ThreeFeatures = X_train[ThreeNames]\n",
    "    \n",
    "    gbr_2 = GradientBoostingRegressor()        \n",
    "    gbr_2.fit(ThreeFeatures, y_train.values.ravel())\n",
    "    ThreeInteraction = h(gbr_2, ThreeFeatures)\n",
    "    if ThreeInteraction>0.2:\n",
    "        print(ThreeNames, ' --> ', ThreeInteraction )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected Features to Predict Binary DeltaAdaptation\n",
    "---\n",
    "\n",
    "Single Features:\n",
    "- AdaptationDuration\n",
    "\n",
    "Two-Way Feature Interactions:\n",
    "- {('SpeedDifference', 'MidSpeed'): 0.2517395510368736}\n",
    "\n",
    "Three-Way Feature Interactions:\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
